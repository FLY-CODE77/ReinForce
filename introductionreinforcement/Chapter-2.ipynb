{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Solution Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-arm Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most important feature of reinforcement learing is \n",
    "    - Training info \n",
    "    - that evaluates the actions taken\n",
    "    - Rather than instructs by giving correct actions\n",
    "---\n",
    "- Purely evaluative feedback indicates how good the action taken \n",
    "- Not action is the best or worst action possible\n",
    "---\n",
    "- Evaluative feed back is the basis of methods for finction optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The particular nonassociative,evaluative feedback problem \n",
    "- that we explore is a simple version of the n-armed bandit problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An n-Armed Bandit Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each action has an expected or mean reward given action is selected : value of action\n",
    "- If we knew the value of each action, it would be trivial to solce the n-armed bandit machine \n",
    "- Select always highest value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We assume that we don't know the action values with certainty, although we may have estimates\n",
    "\n",
    "#### exploiting, exploring \n",
    "- if we maintain estimates of the action values\n",
    "- any time step there is at least one action whose estimated value is greatest \n",
    "- We call this a greedy action \n",
    "---\n",
    "- If i select a greedy action, we can say we are exploiting(착취) my current knowledge of the values of actions\n",
    "- If we instead select one of nongreedy action then we called exploring\n",
    "    - this enables me to imporve my estimate of the nongreedy action's values \n",
    "    - exploration may produce the greater total reward in the long run "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-value Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- look more closely \n",
    "- simple methods for estimating the values of actions &\n",
    "- for using the estimates to make action selection decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We denote the true(actual) value of action a as q(a)\n",
    "- estimated value on the tth time step as $Q_t(a)$\n",
    "- true value of an action is mean reward when that action is selected \n",
    "---\n",
    "- One natural way to estimate True value of action is by averaging the rewards actually reveived when action selected\n",
    "---\n",
    "- In other words \n",
    "- if by the tth time step action a has been chosen $N_t(a)$ times prior ot t, \n",
    "- yielding rewards $R_1,R_2,...R_{N_t(a)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q_t(a) = \\frac{R_1 + R_2 + --- + R_{N_t(a)}}{N_t(a)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $N_t(a) = 0$ then we define $Q_t(a)$ instead as some default value,\n",
    "- Such as $Q_1(a)$ =0\n",
    "---\n",
    "- As $N_t(a) --> \\infty$, bu the law of large numbers\n",
    "- $Q_t(a)$ converges(수렴) to a q(a)\n",
    "- --\n",
    "- We call this the sample average method for estimating action values \n",
    "- because each estimate is a simple average of the sample of relevant rewards "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The simplest action selection rule is\n",
    "- select the action with highest estimated action value\n",
    "- to select at step t one of the greedy actions, $A_{t}^{*}= max_aQ_t(A)$\n",
    "#### greedy action selection method \n",
    "$$ A_t = argmax_aQ_t(a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- where $argmax_a$ denotes the value if a at which the expression that follows maximized \n",
    "--- \n",
    "\n",
    "- Greedy action selection always exploits(악용) \n",
    "    - current knowledge to maximize immediate reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Greedy selection \n",
    "    + spends no time at all sampling apparently inferior action \n",
    "    + to see if they meight really be better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative is near-greedy action selection rule $\\epsilon$-greedy methods\n",
    "- behave greedily most of time\n",
    "- but once at a time select randomly from amongst all the actions with \n",
    "- equal probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- near greedy action's advantage\n",
    "- In the limit as the number of plays increases\n",
    "- every action will be sampled an infinite number of times\n",
    "- guaranteeing tha $N_t(a) -> \\infty$ for all a \n",
    "- ensuring that all the $Q_t(a)$ converage q(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roygly access the relative effectiveness of greedy and $\\epsilon$ greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$ is better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://blog.kakaocdn.net/dn/cUuiO7/btq4iNI1KW7/axt3vIkKkELLJFlNjHoXP0/img.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The action-value methods\n",
    "    - All estimate action values as sample averages of observed rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When the estimate of the value of action\n",
    "\n",
    "- $$Q_t(a) = \\frac{R_1 + R_2 + R_3+ ...+R_{N_t(a)}}{N_t(a)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $R_1 + ,,,+ R_n$ are all the rewards received  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A problem with this straightforward implementation\n",
    "- memory and computational requirement grow over time with out bound\n",
    "---\n",
    "- each additional reward following a selection of action a need more memory\n",
    "    - to determine $Q_t(a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to row the memory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://blog.kakaocdn.net/dn/cUCtSP/btq4c0QD3tk/QD7opVceWk3yNgyJ9YIVE0/img.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- New Estimate <- OldEstimate + StepSize[Target-OldEstimate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Tartget - OldEstimate] is error in the estimate \n",
    "- step-size paramter by symbol $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking a Nonstationary Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The average method are appropriate in stationary env\n",
    "---\n",
    "- But not if the bandit is changing over time\n",
    "- We often encounter reinforcement learning problems that are effectively nonstationanry \n",
    "---\n",
    "- in nonstationary problem recent reward more heavily weight than long-past ageo\n",
    "- One of most popular ways of doing this is use constant step-size params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://blog.kakaocdn.net/dn/RbOwT/btq4di44Okr/kuEp3YM9lkzihvqD42ifDK/img.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where step-size parameter $\\alpha {\\in} (0,1]$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We call this a weighted average \n",
    "- The quantity 1 - $\\alpha$ is less than 1\n",
    "    - thus the weight of R decrease as the number of intervening(개입) rewards increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimistic Initial Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All methods we have discussed are dependent to extent on the initial action-value extimates $Q_1$\n",
    "- language of statistics, these methods are biased by their initial estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper-Confidence-Bound Action Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploration is needed \n",
    "    - estimates of the action value are uncertain\n",
    "\n",
    "- greedy actions are best at present, but some of the other actions may actually be better\n",
    "- $\\epsilon$-greedy action selection forces the non-greedy action to tried \n",
    "- but with no preference for those that are nearly greedy or paricularly uncertain\n",
    "- better if select among the non-greddy actions according to potentail for actually optimal\n",
    "---\n",
    "- UCB \n",
    "$ A_t = argmax_a[Q_t(a) + c \\sqrt{\\frac{lnt}{n_t(a)}}]$\n",
    "- it perform better than $ \\epsilon$-greedy action \n",
    "- but some difficulty when dealing with large state spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this chapter several ways of balancing exploartion and exploitaion\n",
    "---\n",
    "- The $\\epsilon$-greedy methods \n",
    "    - choose randomly a small fraction of the time\n",
    "---\n",
    "- UCB methods \n",
    "    - choose deterministically(결정론적으로) achieve exploration by \n",
    "    - subtly favoring at each step the actions that have so far received fewer samples\n",
    "---\n",
    "- Gradient bandit algorithms \n",
    "    - estimate not action values\n",
    "    - but action preferences\n",
    "    - And favor more preferred actions in graded \n",
    "    - probabalistic manner using a soft-max distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
