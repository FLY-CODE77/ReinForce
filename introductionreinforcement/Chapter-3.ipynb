{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The reinforcement learning problem \n",
    "    - straightforward framing of the problem of learning from interaction to achieve goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The learner and decision-maker is called the agent\n",
    "- environment is comprising everything outside the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interaction\n",
    "- agent selecting actions and the env reponding to those actions \n",
    "    - presenting new situations to the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- env also give rsie to rewards \n",
    "- Values that the agent tries to maximize over time \n",
    "- task : complete enviroment defines a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### environment's stats $S_t$\n",
    "- set of possible states \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### agent policy $\\pi_t$\n",
    "- agent implements a mapping from states to probabilities of selecting each possible action\n",
    "---\n",
    "- where $\\pi_t(a|s)$ is probability that $A_t =a$ if $S_t = s$\n",
    "- agent goal is maximize the total amount of reward it receives over the long run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This framework abstract(요약) and flexible and can be applied to many diffierent problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals and Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In reinforcement learning \n",
    "- The goal of agents is formalized(형식) : special reward passing from environment to the agent \n",
    "- at each time step : reward is simple number \n",
    "- the agent golal is maximize the total amount of reward it receives\n",
    "- Not maximizing immediate reward but cumulatrive reward it receives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use of a reward signal to formalize the idea of goal is \n",
    "- most distinctive features of reinforcement learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so far we have discussed the objective of learning informally \n",
    "- How do we might defined formally \n",
    "---\n",
    "- if sequence of reward receive after time step t is denoted \n",
    "- $R_{t+1}, R+{t+2} , ...$\n",
    "- when what precies aspect of this sequence do we wich to maximize\n",
    "- $G_t = R_{t+1} + R_{t+2} + R_{t+3} + ... R_T$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when the agent-enviroemnt interaction breaks naturally into subsequences \n",
    "- we call this episodes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each episode ends in a special stats called the terminal state\n",
    "- S : mean nonterminal states\n",
    "- $S^+$ : mean terminal states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- on the other hand, in many cases the agent-env interaction does not break natuarlly into identifiable episodes\n",
    "- but goes on continually without limit \n",
    "---\n",
    "- We call this continual process-control task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in Continual process control task receive reward inf\n",
    "- so we can't maximize  so little math add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- additional concept that of discounting \n",
    "- According to this approach, the agent tries to select actions so \n",
    "- that the sum of the discounted rewards it receives over the future is maximize \n",
    "---\n",
    "- it chooses $A_t$ to maximize the expected discounted return \n",
    "- $G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$\n",
    "- $gamma$ is parameter, $0 <= \\gamma <= 1$ , discount rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- discount rate determines the present value of future rewards \n",
    "- a reward received k time steps in the future is worth only $\\gamma^{k-1}$ times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- $\\gamma <1$, the infinite sum has as finite value as long as the reward sequence $R_k$ is bounded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\gamma$ = 0 the agent is myopic(근시) is concerned only with maximizing immediate rewards \n",
    "- action influence only the immediate reward not future rewards as well \n",
    "- myopic agent then maximizing each reward ( but it is not goood option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Notation for Episodic and Continuing Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we talking about \n",
    "    - episodic task : interaction breaks down into a sequence of separate\n",
    "    - continuing task : does not end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- episodic tasks consider of episodes each : consis of finity seqence \n",
    "- $S_{t,i}$ - state representation t : time, i : epiodes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Markov Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the agent makes decisions as a function of a signal from the evironment called : environments state\n",
    "- We will learn\n",
    "- what is required of the state signal \n",
    "- what kind of info we should give or not\n",
    "#### Markov property\n",
    "- property of env and their state signals that is of particular interest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"the state\" we mean whatever info is avilable to the agent\n",
    "- The state signal should not be expected to inform the agent of everything about the enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
